<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SSFPN]]></title>
    <url>%2F2022%2F06%2F28%2FSSFPN%2F</url>
    <content type="text"><![CDATA[SSFPN论文地址：https://arxiv.org/abs/2206.07298 论文中项目地址：https://github.com/mohamedac29/S2-FPN/ 自己实现的项目地址：https://github.com/llfzllfz/DL_Exercise/tree/main/SSFPN 模型纵览 上面两个模型讲述了SSFPN的模型总体结构。 Overview根据上面图片中的描述，就可以得到模型的总体结构，模型分为三个部分： 特征抽取或编码 注意力金字塔融合（APF） 全局特征上采样（GFU） 其中分别包括了CFGB，FAB，SSAM。 SSFPN采用resnet18或者resnet34作为基础的model，去掉其中的全局平均池化层以及softmax，根据基础模型的步长取出对应的特征。步长为2，4，8，16，32的情况下，得到F1，F2，F3，F4，F5。 在F5之后分别接两个模块： 粗糙特征生成块（CFGB） 包含一个步长为2的卷积层来生成粗糙特征，目的是为了APF做准备 特征适应块（FAB） 包含一个步长为1的卷积层来为GFU做准备 注意：APF中步长分别为4，8，16，32。 APF2，APF3，APF4，APF5是从上到下的特征生成，使用APF模块。 Code-SSFPN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384class SSFPN(nn.Module): def __init__(self, backbone, classes = 1, pretrained = True): super().__init__() self.backbone = backbone.lower() self.pretrained = pretrained self.encoder, self.out_channels = self.get_backbone_layer() self.conv1_x = self.encoder.conv1 self.bn1 = self.encoder.bn1 self.relu = self.encoder.relu self.maxpool = self.encoder.maxpool self.conv2_x = self.encoder.layer1 self.conv3_x = self.encoder.layer2 self.conv4_x = self.encoder.layer3 self.conv5_x = self.encoder.layer4 self.fab = nn.Sequential( conv_block(in_channels=self.out_channels, out_channels=self.out_channels // 2, kernel_size=3, stride=1, padding=1, use_bn_act=True) ) self.cfgb = nn.Sequential( conv_block(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=2, padding=1, use_bn_act=True) ) self.apf5 = APF(self.out_channels, self.out_channels, self.out_channels // 2, classes=classes) self.apf4 = APF(self.out_channels // 2, self.out_channels // 2, self.out_channels // 4, classes=classes) self.apf3 = APF(self.out_channels // 4, self.out_channels // 4, self.out_channels // 8, classes=classes) self.apf2 = APF(self.out_channels // 8, self.out_channels // 8, self.out_channels // 16, classes=classes) self.gfu5 = GFU(self.out_channels // 2, self.out_channels // 2, self.out_channels // 2) self.gfu4 = GFU(self.out_channels // 4, self.out_channels // 2, self.out_channels // 4) self.gfu3 = GFU(self.out_channels // 8, self.out_channels // 4, self.out_channels // 8) self.gfu2 = GFU(self.out_channels // 16, self.out_channels // 8, self.out_channels // 16) self.classifier = conv_block(self.out_channels // 16, classes, 1, 1, 0, True) def get_backbone_layer(self): assert self.backbone == 'resnet18' or self.backbone == 'resnet34' or self.backbone == 'resnet50', f'backbone 不符合' if self.backbone == 'resnet18': encoder = resnet18(pretrained=self.pretrained) out_channels = 512 if self.backbone == 'resnet34': encoder = resnet34(pretrained=self.pretrained) out_channels = 512 if self.backbone == 'resnet50': encoder = resnet50(pretrained=self.pretrained) out_channels = 2048 return encoder, out_channels def forward(self, x): B, C, H, W = x.size() x = self.conv1_x(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x2 = self.conv2_x(x) x3 = self.conv3_x(x2) x4 = self.conv4_x(x3) x5 = self.conv5_x(x4) cfgb = self.cfgb(x5) fab = self.fab(x5) apf5, cls5 = self.apf5(cfgb, x5) apf4, cls4 = self.apf4(apf5, x4) apf3, cls3 = self.apf3(apf4, x3) apf2, cls2 = self.apf2(apf3, x2) gfu5 = self.gfu5(apf5, fab) gfu4 = self.gfu4(apf4, gfu5) gfu3 = self.gfu3(apf3, gfu4) gfu2 = self.gfu2(apf2, gfu3) cls = self.classifier(gfu2) pre = F.interpolate(cls, size=(H,W), mode='bilinear') sup5 = F.interpolate(cls5, size=(H,W), mode='bilinear') sup4 = F.interpolate(cls4, size=(H,W), mode='bilinear') sup3 = F.interpolate(cls3, size=(H,W), mode='bilinear') sup2 = F.interpolate(cls2, size=(H,W), mode='bilinear') if self.training: return pre, sup5, sup4, sup3, sup2 else: return pre 比例感知注意力模块（SSAM） SSAM模块见上方图形，其目的是为了获得长范围的依赖关系以及减少计算资源的消耗。 首先用平均池化层（Avg Pool）和最大池化层（Max Pool）分别提取特征，然后过一个权重共享的1*1的卷积层得到F1和F2。 其中在平均池化和最大池化的时候是按照行进行池化，最后得到(C, H, 1)的size，但是在论文作者的实现中，采用的是按列池化，本项目采用的是按行池化。 利用F1和F2得到Attention部分，也就是A＝F１⊙F２ 然后Attention再过一个softmax 最后得到$F_{scale} = A ⊙ F_1 + A ⊙ F_2$ SSAM的最后输出为$F_{SSAM} = \alpha F_{scale} + (1 - \alpha)F$ 原作者在$F_{scale}$之后又过了一个卷积层，但是在论文中并没有看到相应的描述。 Code-SSAM123456789101112131415161718192021222324class SSAM(nn.Module): def __init__(self, in_channels, out_channels): super(SSAM, self).__init__() self.conv_shared = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1) self.bn_shared_max = nn.BatchNorm2d(in_channels) self.bn_shared_avg = nn.BatchNorm2d(in_channels) self.gamma = nn.Parameter(torch.zeros(1)) def forward(self, x): B, C, H, W = x.size() max_pool = F.max_pool2d(x, [1, W]) max_pool = self.conv_shared(max_pool) max_pool = self.bn_shared_max(max_pool) avg_pool = F.avg_pool2d(x, [1, W]) avg_pool = self.conv_shared(avg_pool) avg_pool = self.bn_shared_avg(avg_pool) att = torch.softmax(torch.mul(max_pool, avg_pool), 1) f_scale = att * max_pool + att * avg_pool out = F.relu(self.gamma * f_scale + (1 - self.gamma) * x) return out 注意力金字塔融合模块（APF） APF模块有两个输入，一个是顶层APF的输入($F_{i-1}$)，一个是来自侧层F的输入($F_i$) $F_{i-1 }$先通过一个带normalization和relu的1*1卷积层，$F_i$通过一个上采样层来适应$F_{ i-1 }$ 然后再通过concat将这两个输出拼接起来得到$F_{concat}$，同时将这个输入到一个FRB中得到$F_{R}$ FRB里面包含了1*1和3*3的卷积层，都带着normalization和relu。 接下去，将$F_{ i-1 }$经过一个带normalization和relu的3*3的卷积层。 $F_{R}$经过一个CAM再与$F_{i -1}$相乘得到输出$F_{A}$ $F_{R}$经过一个SSAM与经过一个3*3卷积层的$F_{i}$相乘得到输出$F_{B}$ 最后得到输出$F_{out} = F_{A} + F_{ B}$ $F_{out}$之后接3*3的卷积层进入到相应的处理模块，例如pre和下一个APF以及相应的GFU Code-CAM1234567891011121314151617class CAM(nn.Module): def __init__(self, channel): super().__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.conv1 = conv_block(channel, channel, 1, 1, use_bn_act=False, padding=0) self.relu = nn.ReLU() self.conv2 = conv_block(channel, channel, 1, 1, use_bn_act=False, padding=0) self.sigmoid = nn.Sigmoid() def forward(self, x): redisual = x x = self.avg_pool(x) x = self.conv1(x) x = self.relu(x) x = self.conv2(x) x = self.sigmoid(x) return torch.mul(redisual, x) Code-APF123456789101112131415161718192021222324252627class APF(nn.Module): def __init__(self, channels_high, channels_low, channel_out, classes = 1): super().__init__() self.lateral_low = conv_block(channels_low, channels_high, 1, 1, 0, use_bn_act=True) self.frb = nn.Sequential( conv_block(channels_high * 2, channel_out, 1, 1, 0, True), conv_block(channel_out, channel_out, 3, 1, 1, True) ) self.fc_conv = conv_block(channels_high, channel_out, 3, 1, 1, True) self.fs_conv = conv_block(channels_high, channel_out, 3, 1, 1, True) self.cam = CAM(channel_out) self.ssam = SSAM(channel_out, channel_out) self.classifier = conv_block(channel_out, classes, 3, 1, 1, True) self.apf = conv_block(channel_out, channel_out, 3, 1, 1, True) def forward(self, x_high, x_low): x_low = self.lateral_low(x_low) x_high = F.interpolate(x_high, size=x_low.size()[2:], mode='bilinear') f_c = torch.cat([x_low, x_high], 1) f_r = self.frb(f_c) f_a = torch.mul(self.fc_conv(x_low), self.cam(f_r)) f_b = torch.mul(self.fs_conv(x_high), self.ssam(f_r)) f_out = f_a + f_b apf = self.apf(f_out) classifier = self.classifier(f_out) return apf, classifier 全局特征上采样（GFU）在模型纵览中的第一张图片中，APF后面接的是Predict，但是在第二张图中，显示的是GFU，同时该模型会输出5个输出。 其中每个APF后各有一个输出，GFU后还有一个输出，做推理的时候用的是GFU后的输出。 GFU的模型整理来说相对简单。 FAB经过一个上采样，然后过一个1*1的卷积再做relu和平均池化得到$X_{F}$的输出 APF经过一个1*1的卷积得到$X_P$的输出 最后得到$X_{GFU} = （f^1（X_F+ X_P））$，其中$f^1$表示经过一个1*1的卷积层 Code-GFU12345678910111213141516171819class GFU(nn.Module): def __init__(self, apf_channel, fab_channel, out_channel): super().__init__() self.apf_conv = conv_block(apf_channel, out_channel, 1, 1, 0, True) self.fab_conv = nn.Sequential( conv_block(fab_channel, out_channel, 1, 1, 0, False), nn.ReLU(), nn.AdaptiveAvgPool2d(1) ) self.out_conv = conv_block(out_channel, out_channel, 1, 1, 0, True) def forward(self, apf, fab): B, C, H, W = apf.size() apf = self.apf_conv(apf) fab = F.interpolate(fab, size=(H, W), mode='bilinear') fab = self.fab_conv(fab) f_out = apf + fab f_out = self.out_conv(f_out) return f_out Code-Conv_block1234567891011121314class conv_block(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_bn_act): super(conv_block, self).__init__() self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding) self.relu = nn.ReLU() self.normalization = nn.BatchNorm2d(out_channels) self.use_bn_act = use_bn_act def forward(self, x): if self.use_bn_act: return self.relu(self.normalization(self.conv(x))) else: return self.conv(x) 实践数据集：Baidu People segmentation dataset数据集参考来源：https://blog.csdn.net/MOU_IT/article/details/82225505 图片显示： 第一张显示的是原图片，第二张显示的是分割后的图片。 因为受到机器限制以及数据集像素不统一的问题，因此统一resize成了(224, 224)的大小。 结果 这是根据config里面的参数train10轮得到的结果，并没有完全run完。 接下去利用val中得到的最好成绩的模型进行预测 以上左图为原图，右图为分割图 上面这幅图为模型预测的图，根据对比，可以发现，已经大部分相似了。 可以肯定，如果多训练一段时间，会得到更好的结果。]]></content>
      <tags>
        <tag>DL_Net_Exercise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeNet]]></title>
    <url>%2F2022%2F04%2F22%2FLeNet%2F</url>
    <content type="text"><![CDATA[LeNet结构LeNet最初是为Mnist，其结构较为简单，只有五层，其论文原图如下，论文参考为：Gradient-Based_Learning_Applied_to_Document_Recognition 根据自己的理解得到如下结构，我采用的数据集是Cifar-10，大小为32*32（当然具体的参数可以自己修改的，但是总体是这个意思）： 借助pytorch可视化结果如下： 其解释如下： 首先是卷积部分，也就是卷积+池化的结构 第一个卷积层采用20个大小为5*5的卷积核，stride为1，卷积结果得到20*28*28 第一个池化层采用核大小为2*2的Max Pooling， stride为2，池化结果得到20*14*14 第二个卷积层采用50个大小为5*5的卷积核，stride为2，卷积结果得到50*10*10 第二个池化层采用核大小为2*2的Max Pooling，stride为2，池化结果得到50*5*5 接下去是FC层 第一个全连接层的神经元个数是500，之后接入ReLU激活函数增加非线性能力 第二个全连接层的神经元个数为10个，之后输出到softmax函数中，计算每个类别的得分值，完成分类 代码实现在上面的结构上，FC层中增加了一个drop层 1234567891011121314151617181920212223class LeNet(torch.nn.Module): def __init__(self): super().__init__() # Input: 3*32*32, conv1 output: 20*28*28 self.conv1 = torch.nn.Conv2d(3, 20, kernel_size = (5,5), stride = 1) # 20*28*28-&gt;20*14*14 self.pool1 = torch.nn.MaxPool2d(2) # 20*14*14-&gt;50*10*10 self.conv2 = torch.nn.Conv2d(20, 50, kernel_size = (5,5), stride = 1) # 50*10*10-&gt;50*5*5 self.pool2 = torch.nn.MaxPool2d(2) # 50*5*5-&gt;500 self.fc1 = torch.nn.Linear(1250, 500) self.relu1 = torch.nn.ReLU() self.fc2 = torch.nn.Linear(500, 10) self.relu2 = torch.nn.ReLU() self.dropout = torch.nn.Dropout(0.5) def forward(self, x): x = self.pool1(self.conv1(x)) x = self.pool2(self.conv2(x)) x = self.relu1(self.fc1(x.view(-1,1250))) x = self.fc2(x) return x 结果该网络运行在Cifar-10数据集上的结果如下所示(batch_size = 256, epochs = 50)：]]></content>
      <tags>
        <tag>DL_Net_Exercise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic]]></title>
    <url>%2F2020%2F03%2F26%2FTitanic%2F</url>
    <content type="text"><![CDATA[data_prepared观察数据，进行数据分析 123train = pd.read_csv(r'train.csv')pd.set_option('display.max_columns',None)print(train.head(10)) PassengerId (ID)共891个不同值，因此无价值，可以考虑删除 Survived (是否生存)很明显的label标签，也是我们需要预测的对象 1train['Survived'].value_counts().plot.bar(title = 'Survived') Pclass(船舱等级)生成一张关于Pclass的生存人数图看看 1234567Survived_Pclass = train[train.Survived==1].groupby(train.Pclass).Survived.count()Not_Survived_Pclass = train[train.Survived==0].groupby(train.Pclass).Survived.count()plt.subplot(1,2,1)Survived_Pclass.plot.bar(title='Survived Pclass')plt.subplot(1,2,2)Not_Survived_Pclass.plot.bar(title='Not Survived Pclass')plt.show() 发现Pclassd对于存活率还是有比较大的影响的，因此我们可以通过计算该特征的IV判断是否选择 经过计算，该特征的IV为：0.5006200325081741，因此使用该特征 Name(姓名)通过数据集发现，Name看上去非常的杂乱无章，因为每个人的名字都可能不同，但是仔细研究，依旧可以尝试提取Name中的特征。 例如第一个Name：Braund, Mr. Owen Harris ,可以尝试提取其中的 Mr. 作为特征 123456789train['Name'] = train['Name'].str.split('.').str.get(0)train['Name'] = train['Name'].str.split(',').str.get(1)Survived_Name = train[train.Survived == 1].groupby(train.Name).Survived.count()Not_Survived_Name = train[train.Survived == 0].groupby(train.Name).Survived.count()plt.subplot(1,2,1)Survived_Name.plot.bar(title = 'Survived Name')plt.subplot(1,2,2)Not_Survived_Name.plot.bar(title = 'Not Survived Name')plt.show() 粗略一看其实也是有关联的，例如Miss存活率就比较高，接下来计算IV值 仔细观察可以发现有些值不是两者都有的，对于这些值我们是无法计算WOE的，因此首先去除掉这些值： 123456789101112131415161718192021train_name = train.copy()for idx in range(len(train_name)): if train.Survived[idx] == 1: if train.Name[idx] not in Not_Survived_Name: train_name.drop(index = idx,inplace = True) else: if train.Name[idx] not in Survived_Name: train_name.drop(index = idx,inplace = True) for x in Survived_Name.index: if x not in Not_Survived_Name.index: Survived_Name.drop(x,inplace = True)for x in Not_Survived_Name.index: if x not in Survived_Name.index: Not_Survived_Name.drop(x,inplace = True)plt.subplot(1,2,1)Survived_Name.plot.bar(title = 'Survived Name')plt.subplot(1,2,2)Not_Survived_Name.plot.bar(title = 'Not Survived Name')plt.show() 然后通过这个再去计算IV值：1.526927503266331 Sex（性别）生成一张关于性别的生存人数图看看： 1234567Survived_Sex = train[train.Survived==1].groupby(train.Sex).Survived.count()Not_Survived_Sex = train[train.Survived==0].groupby(train.Sex).Survived.count()plt.subplot(1,2,1)Survived_Sex.plot.bar(title='Survived Pclass')plt.subplot(1,2,2)Not_Survived_Sex.plot.bar(title='Not Survived Pclass')plt.show() 很明显看到女性的存活率比男性高，再来算算IV值：1.3371613282771198 Age(年龄)因为Age是一个连续的变量，因此如果直接采用柱状图会导致无法分析，因此先对Age进行分组，令每10年为一段，共分成10段，对这10段进行分析： 12345678910train.dropna(inplace = True,subset = ['Age'])train['Age_group'] = train['Age']/10 + 1train['Age_group'] = train['Age_group'].astype(int)Survived_Age = train[train.Survived==1].groupby(train.Age_group).Survived.count()Not_Survived_Age = train[train.Survived==0].groupby(train.Age_group).Survived.count()plt.subplot(1,2,1)Survived_Age.plot.bar(title='Survived Age',rot=0)plt.subplot(1,2,2)Not_Survived_Age.plot.bar(title='Not Survived Age',rot=0)plt.show() 发现Age对Survived影响还是挺大的，再来看看IV值：0.3074963748595815，可以选择此特征值 SibSp(旁系) 1234567Survived_SibSp = train[train.Survived==1].groupby(train.SibSp).Survived.count()Not_Survived_SibSp = train[train.Survived==0].groupby(train.SibSp).Survived.count()Survived_SibSp.plot.line(label='Survived SibSp',rot=0)Not_Survived_SibSp.plot.line(label='Not Survived SibSp',rot=0)plt.legend(loc = 'upper right')plt.title('SibSp in Survived')plt.show() IV：0.14091013517926007 Parch(直系亲友) IV:0.1147797445933584 Ticket(票编号)原以为看数据觉得Ticket应该是一个无用的数据，但是作图一分析，居然可以当作有用信息： 虽然大部分的编号是不一样的，但是依旧有一部分的编号是相同的，选择其中既有正例又有反例的部分做IV，课可以得到IV值：0.2421326661022386 不过经过这么剔除后，原数据大小891就变成了127条，损失太多，是否使用有待考虑。 Fare(票价)在这个数据集中，票价最高512.3292，最低0，共有248种不同的票价，将这些票价分成25组再作图： (分成25份分法：最高最低值均分25份) 12train['Fare_group'] = train['Fare']/((train['Fare'].max()+0.001)/25) + 1train['Fare_group'] = train['Fare_group'].astype(int) 发现票价越低存活率越低，大胆猜测票价可能受到船舱等级的影响 查图，果然票价越低，乘坐Pclass=3的船舱的概率越大 Cabin(客舱编号)这个属性的数据缺失严重，所有891条数据中，只有204条数据是有效的，其余数据均缺失，暂时不采用该数据 Embarked(上船的港口编号) IV:0.12278149648986617 这个属性相对于别的属性利用价值偏小 缺失值处理在处理整个数据的时候，发现有些数据有残缺，需要进行缺失值处理： V1对于Age参数，采用平均值填充， 对于Cabin，取消该列， 对于Embarked，暂时发现价值偏小，因此暂时取消该值 V2在处理缺失值之前，先总结一下V1版本： · 首先是Age的处理，直接采用平均值太过暴力 · 其次是Cabin的处理和Embarked的处理，直接取消会不会产生影响 · 对于Name属性的编码出现了错误，直接使用 lab.fit_transform 会产生编码错误 · 对于SibSp 属性和 Parch 属性，感觉在一定程度上有重复 优化1（Age）相对于V1来说，Age直接采用平均值填充未免太过暴力，因此考虑别的填充方式。 首先我们需要查看Age和别的属性之间的关系： (Sex只有0和1两个属性)根据上图可以发现，在每个年龄段，Sex为1的人数都大于Sex为0的人数 虽然这张图看上去很杂乱，但是还是可以发现一些规律 因为Fare是连续值，因此将Fare分成25份之后再查看Age的分布情况： 因此考虑使用预测的方式来对Age进行填充。 在这个版本中采用对Age分组，然后采用分类的方式（或许以后可以考虑回归的版本）。 优化2（Cabin）对于Cabin属性，缺失值确实太多了，如果把Cabin属性中的缺失部分记为X，非缺失部分取其首位字母，作图看看： 预估IV：0.4675285868289105 因此可以尝试将缺失值如此处理 优化3（Embarked）对于Embarked，也尝试使用，因缺失值较少，采取随机赋值的方式 优化4（Name编码）提取出Name中的所有不同值，依次进行编码，如果test中出现了train以外的，统一归为一类 （还有个问题：在name中有些name的个数为1，这样的样本是否可以结合起来） 针对上面这个问题，采取：将train中出现个数为个位数的归为一类，和test出现了train以外的归在一起 优化5（SibSp 和 Parch）在这方面采用多加一个Family_size属性，其值为SibSp和Parch之和 优化6（Name属性提取）在Name属性中提取了Mr.的内容，但是Name属性中还有姓氏，提取出姓氏形成新的属性Surname 在新属性Surname中，采用二分类，具有相同姓氏的归为1，否则为0 优化7（模型优化）同时在模型的实现方面，一方面采用随机森林对所有属性进行训练，同时训练多个随机森林，对多种属性组和进行训练，最后按照不同的权重比例取值 V3· 有了Family_size之后感觉SibSp和Parch有点多余，因此选择删除 · Cabin选择缺失值插入X，Cabin损失值太多，最终还是选择放弃该属性 · 修改了一下随机森林训练的属性 实现V1采用随机森林，缺失值处理采用V1的处理方式，采用的特征组合为：Pclass,Name,Sex,Age,SibSp,Parch,Fare 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import pandas as pd#导入数据文件import numpy as np#科学计算计算库import seaborn as snsimport matplotlib.pyplot as plt#数据可视化库import warningsimport pandas_profiling as ppf#edafrom sklearn.preprocessing import LabelEncoder#标签编码from sklearn.preprocessing import MinMaxScaler#归一化from sklearn.model_selection import train_test_split#数据集的划分from sklearn.linear_model import LinearRegression#算法from sklearn.metrics import mean_absolute_error#评估函数from pandas import DataFrame as dffrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import roc_auc_scoredef delete_data(train,test,name): train.drop(name,axis=1,inplace = True) test.drop(name,axis=1,inplace = True ) return train,testdef lab_data(lab,train,test,name): train[name] = lab.fit_transform(train[name]) test[name] = lab.fit_transform(test[name]) return train,testdef minmax_data(minmax,train,test,name): train[name] = minmax.fit_transform(np.array(train[name]).reshape(-1,1)) test[name] = minmax.fit_transform(np.array(test[name]).reshape(-1,1)) return train,testif __name__ == '__main__': train = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\train.csv') test = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\test.csv') #pd.set_option('display.max_columns',None) train['Name'] = train['Name'].str.split('.').str.get(0) train['Name'] = train['Name'].str.split(',').str.get(1) test['Name'] = test['Name'].str.split('.').str.get(0) test['Name'] = test['Name'].str.split(',').str.get(1) train['Age'] = train['Age'].fillna(np.mean(train['Age'])) test['Age'] = test['Age'].fillna(np.mean(test['Age'])) PassengerId = test.PassengerId train,test = delete_data(train,test,'Cabin') train,test = delete_data(train,test,'Embarked') train,test = delete_data(train,test,'Ticket') train,test = delete_data(train,test,'PassengerId') lab = LabelEncoder() train,test = lab_data(lab,train,test,'Sex') train,test = lab_data(lab,train,test,'Name') test['Fare'] = test['Fare'].fillna(np.mean(test['Fare'])) minmax = MinMaxScaler() train,test = minmax_data(minmax,train,test,'Fare') train,test = minmax_data(minmax,train,test,'Age') #train.info() x = train.drop('Survived',axis = 1) y = train['Survived'] x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 10) #print(x_train.shape,y_train.shape) model = RandomForestClassifier(n_estimators=100, max_features='sqrt') model.fit(x_train,y_train) rf_predictions = model.predict(x_test) #print(rf_predictions) print(model.score(x_test,y_test)) test_predictions = model.predict(test) submission = pd.DataFrame(&#123; 'PassengerId':PassengerId, 'Survived':test_predictions&#125;) submission.to_csv('V1.1.csv',index = False) 该实验在kaggle上面评分0.71291 V2对于Age的预测采用的属性：Pclass，Name，Sex, SibSP, Parch, Fare_group, Family_size,Embarked,Surname,Cabin 采用多个随机森林按照不同的权重取值，按照不同的属性组合进行随机森林的训练 随机森林训练： 123456789101112131415161718192021222324['Pclass','Fare'],['Pclass','Fare_group'],['SibSp','Parch'],['SibSp','Parch','Family_size'],['Age_group','SibSp','Parch','Family_size'],['Age_group','Name','Surname'],['SibSp','Parch','Family_size','Surname','Name'],['Cabin','Embarked'],['Fare_group','Fare'],['Pclass','Age_group','Fare','Fare_group'],['Surname','Name'],['Pclass','SibSp','Parch','Family_size'],[all - 'Name'],[all - 'Surname'],[all - 'SibSp'],[all - 'Parch'],[all - 'Cabin'],[all - 'Embarked'],[all - 'Family_size'],[all - 'Fare_group'],[all - 'Age_group'],[all - 'Pclass'],[all - 'Fare'],[all] * 4 共计生成了27个预测结果，采用少数服从多数，阈值大约为18 ~ 19。对于每一个真实的结果，当含有1的个数大于等于阈值时，认为其是可靠的预测结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import pandas as pd#导入数据文件import numpy as np#科学计算计算库import seaborn as snsimport matplotlib.pyplot as plt#数据可视化库import warningsimport pandas_profiling as ppf#edafrom sklearn.preprocessing import LabelEncoder#标签编码from sklearn.preprocessing import MinMaxScaler#归一化from sklearn.model_selection import train_test_split#数据集的划分from sklearn.linear_model import LinearRegression#算法from sklearn.metrics import mean_absolute_error#评估函数from pandas import DataFrame as dffrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import roc_auc_scorefrom sklearn.datasets import make_classificationimport randomdef delete_data(train,test,name): train.drop(name,axis=1,inplace = True) test.drop(name,axis=1,inplace = True ) return train,testdef lab_data(lab,train,test,name): train[name] = lab.fit_transform(train[name]) print(lab.classes_) test[name] = lab.fit_transform(test[name]) print(lab.classes_) return train,testdef minmax_data(minmax,train,test,name): train[name] = minmax.fit_transform(np.array(train[name]).reshape(-1,1)) test[name] = minmax.fit_transform(np.array(test[name]).reshape(-1,1)) return train,testdef ready_data(x,test,name): return x[name],test[name]def RandomForest(x,y,test,ans,idx): clf = RandomForestClassifier(n_estimators = 100) clf.fit(x,y) ans[idx] = clf.predict(test) return ansif __name__ == '__main__': train = pd.read_csv(r'train.csv') test = pd.read_csv(r'test.csv') train_ = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\train.csv') test_ = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\test.csv') test_['Fare'] = test_['Fare'].fillna(np.mean(train_['Fare'])) pd.set_option('display.max_columns',None) train['Fare'] = train_['Fare'] test['Fare'] = test_['Fare'] minmax = MinMaxScaler() train,test = minmax_data(minmax,train,test,'Fare') train_Age = train.copy() train_Age.dropna(inplace = True,subset = ['Age']) train_Age['Age_group'] = train_Age['Age']/10 + 1 train_Age['Age_group'] = train_Age['Age_group'].astype(int) train_Age.drop('Survived',axis = 1,inplace = True) train_Age.drop('PassengerId',axis = 1,inplace = True) train_Age.drop('Age',axis = 1,inplace = True) x_Age = train_Age.drop('Age_group',axis = 1) y_Age = train_Age['Age_group'] model = RandomForestClassifier(n_estimators=100) model.fit(x_Age,y_Age) sums = 0 alls = 0 train['Age_group'] = model.predict(train[['Pclass','Name','Sex','SibSp','Parch','Cabin','Embarked','Fare_group','Surname','Family_size','Fare']]) for idx in range(len(train)): if np.isnan(train['Age'][idx]) == 0: if train['Age_group'][idx] == int(train['Age'][idx] / 10 + 1): sums = sums + 1 train['Age_group'][idx] = int(train['Age'][idx] / 10 + 1) alls = alls + 1 train.drop('Age',axis = 1,inplace = True) train.info() print(sums,alls,sums / alls) test['Age_group'] = model.predict(test[['Pclass','Name','Sex','SibSp','Parch','Cabin','Embarked','Fare_group','Surname','Family_size','Fare']]) for idx in range(len(test)): if np.isnan(test['Age'][idx]) == 0: test['Age_group'][idx] = int(test['Age'][idx] / 10 + 1) test.drop('Age',axis = 1,inplace = True) PassengerId = test['PassengerId'] train,test = delete_data(train,test,'PassengerId') x = train.drop('Survived',axis = 1) y = train['Survived'] x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 10) ans = pd.DataFrame(&#123;'PassengerId':PassengerId&#125;) l = [['Pclass','Fare'], ['Pclass','Fare_group'], ['SibSp','Parch'], ['SibSp','Parch','Family_size'], ['Age_group','SibSp','Parch','Family_size'], ['Age_group','Name','Surname'], ['SibSp','Parch','Family_size','Surname','Name'], ['Cabin','Embarked'], ['Fare_group','Fare'], ['Pclass','Age_group','Fare','Fare_group'], ['Surname','Name'], ['Pclass','SibSp','Parch','Family_size']] x_test = test x_train = x y_train = y for idx in range(len(l)): print(l[idx]) n_x,n_t = ready_data(x_train,x_test,l[idx]) ans = RandomForest(n_x,y_train,n_t,ans,str(idx)) ans = RandomForest(x_train.drop('Name',axis = 1),y_train,x_test.drop('Name',axis = 1),ans,str(len(l))) ans = RandomForest(x_train.drop('Surname',axis = 1),y_train,x_test.drop('Surname',axis = 1),ans,str(len(l) + 1)) ans = RandomForest(x_train.drop('SibSp',axis = 1),y_train,x_test.drop('SibSp',axis = 1),ans,str(len(l) + 2)) ans = RandomForest(x_train.drop('Parch',axis = 1),y_train,x_test.drop('Parch',axis = 1),ans,str(len(l) + 3)) ans = RandomForest(x_train.drop('Cabin',axis = 1),y_train,x_test.drop('Cabin',axis = 1),ans,str(len(l) + 4)) ans = RandomForest(x_train.drop('Embarked',axis = 1),y_train,x_test.drop('Embarked',axis = 1),ans,str(len(l) + 5)) ans = RandomForest(x_train.drop('Family_size',axis = 1),y_train,x_test.drop('Family_size',axis = 1),ans,str(len(l) + 6)) ans = RandomForest(x_train.drop('Fare_group',axis = 1),y_train,x_test.drop('Fare_group',axis = 1),ans,str(len(l) + 7)) ans = RandomForest(x_train.drop('Age_group',axis = 1),y_train,x_test.drop('Age_group',axis = 1),ans,str(len(l) + 8)) ans = RandomForest(x_train.drop('Pclass',axis = 1),y_train,x_test.drop('Pclass',axis = 1),ans,str(len(l) + 9)) ans = RandomForest(x_train.drop('Fare',axis = 1),y_train,x_test.drop('Fare',axis = 1),ans,str(len(l) + 10)) ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 11)) ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 12)) ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 13)) ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 14)) ans['Sum'] = ans.apply(lambda x:x.sum(),axis = 1) ans['Sum'] = ans['Sum'] - ans['PassengerId'] ans['Survived'] = ans['Sum'].apply(lambda x: 1 if x &gt;= 18 else 0) for idx in range(27): ans.drop(str(idx),inplace = True,axis = 1) ans.drop('Sum',inplace = True,axis = 1) print(ans.head(10)) ans.to_csv('V2.X2.csv',index = False) 上述代码部分省略了数据的预处理部分 上述模型在kaggle上为0.799 kaggle上面震荡大约在0.770~0.799(可能再偏低点或偏高点) V3随机森林训练： 123456789101112['Pclass','Fare'],['Pclass','Fare_group'],['Age_group','Family_size'],['Age_group','Name','Surname'],['Family_size','Surname','Name'],['Fare_group','Fare'],['Pclass','Age_group','Fare','Fare_group'],['Surname','Name'],['Pclass','Family_size'],[all - 两种属性的所有组合],[all - 单种属性],[all] * 2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160import pandas as pd#导入数据文件import numpy as np#科学计算计算库import seaborn as snsimport matplotlib.pyplot as plt#数据可视化库import warningsimport pandas_profiling as ppf#edafrom sklearn.preprocessing import LabelEncoder#标签编码from sklearn.preprocessing import MinMaxScaler#归一化from sklearn.model_selection import train_test_split#数据集的划分from sklearn.linear_model import LinearRegression#算法from sklearn.metrics import mean_absolute_error#评估函数from pandas import DataFrame as dffrom sklearn.ensemble import RandomForestClassifierfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.ensemble import ExtraTreesClassifierfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.metrics import roc_auc_scorefrom sklearn.datasets import make_classificationimport randomdef delete_data(train,test,name): train.drop(name,axis=1,inplace = True) test.drop(name,axis=1,inplace = True ) return train,testdef lab_data(lab,train,test,name): train[name] = lab.fit_transform(train[name]) print(lab.classes_) test[name] = lab.fit_transform(test[name]) print(lab.classes_) return train,testdef minmax_data(minmax,train,test,name): train[name] = minmax.fit_transform(np.array(train[name]).reshape(-1,1)) test[name] = minmax.fit_transform(np.array(test[name]).reshape(-1,1)) return train,testdef calcWOE(dataset,col,targe): subdata=df(dataset.groupby(col)[col].count()) suby=df(dataset.groupby(col)[targe].sum()) data=df(pd.merge(subdata,suby,how="left",left_index=True,right_index=True)) b_total=data[targe].sum() total=data[col].sum() g_total=total-b_total data["bad"]=data.apply(lambda x:round(x[targe]/b_total,3),axis=1) data["good"]=data.apply(lambda x:round((x[col]-x[targe])/g_total,3),axis=1) data["WOE"]=data.apply(lambda x:np.log(x.bad/x.good),axis=1) return data.loc[:,["bad","good","WOE"]] def calcIV(dataset): dataset["IV"]=dataset.apply(lambda x:(x.bad-x.good)*x.WOE,axis=1) print(dataset) print(dataset['IV'].sum()) IV=sum(dataset["IV"])def ready_data(x,test,name): return x[name],test[name]def RandomForest(x,y,test,ans,idx): clf = RandomForestClassifier(n_estimators = 100) clf.fit(x,y) ans[idx] = clf.predict(test) return ansdef show_acc(ans,con): ans['Survived'] = ans['Sum'].apply(lambda x: 1 if x &gt;= con else 0) #print(ans.head(10)) sums = 0 alls = 0 for idx in ans.index: if ans['PassengerId'][idx] == ans['Survived'][idx]: sums = sums + 1 alls = alls + 1 print(con,sums,alls,sums/alls)if __name__ == '__main__': train = pd.read_csv(r'trainX.csv') test = pd.read_csv(r'testX.csv') train_ = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\train.csv') test_ = pd.read_csv(r'E:\502\kaggle\get_starts\titanic\data\test.csv') test_['Fare'] = test_['Fare'].fillna(np.mean(train_['Fare'])) #test.info() pd.set_option('display.max_columns',None) #train['Name_size'] = train_['Name'].str.len() #test['Name_size'] = test_['Name'].str.len() train,test = delete_data(train,test,'SibSp') train,test = delete_data(train,test,'Parch') train,test = delete_data(train,test,'Cabin') #train['Alone'] = train['Family_size'].apply(lambda x:1 if x &gt;= 1 else 0) #test['Alone'] = test['Family_size'].apply(lambda x:2 if x &gt;= 1 else 0) train.info() test.info() PassengerId = test['PassengerId'] train,test = delete_data(train,test,'PassengerId') x = train.drop('Survived',axis = 1) y = train['Survived'] x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 10) #print(y_test) ans = pd.DataFrame(&#123;'PassengerId':PassengerId&#125;) lx = [['Pclass','Fare'], ['Pclass','Fare_group'], ['Age_group','Family_size'], ['Age_group','Name','Surname'], ['Family_size','Surname','Name'], ['Fare_group','Fare'], ['Pclass','Age_group','Fare','Fare_group'], ['Surname','Name'], ['Pclass','Family_size']] x_test = test x_train = x y_train = y l = ['Pclass','Name','Sex','Embarked','Surname','Family_size','Fare_group','Fare','Age_group'] icount = 0 for idx in range(len(l)): for idy in range(idx + 1,len(l)): ans = RandomForest(x_train.drop([l[idx],l[idy]],axis = 1),y_train,x_test.drop([l[idx],l[idy]],axis = 1),ans,str(icount)) icount = icount + 1 for idx in range(len(lx)): print(lx[idx]) n_x,n_t = ready_data(x_train,x_test,lx[idx]) ans = RandomForest(n_x,y_train,n_t,ans,str(icount)) icount = icount + 1 ans = RandomForest(x_train.drop('Name',axis = 1),y_train,x_test.drop('Name',axis = 1),ans,str(icount)) ans = RandomForest(x_train.drop('Surname',axis = 1),y_train,x_test.drop('Surname',axis = 1),ans,str(icount + 1)) #ans = RandomForest(x_train.drop('SibSp',axis = 1),y_train,x_test.drop('SibSp',axis = 1),ans,str(len(l) + 2)) #ans = RandomForest(x_train.drop('Alone',axis = 1),y_train,x_test.drop('Alone',axis = 1),ans,str(icount + 11)) #ans = RandomForest(x_train.drop('Name_size',axis = 1),y_train,x_test.drop('Name_size',axis = 1),ans,str(icount + 10)) ans = RandomForest(x_train.drop('Embarked',axis = 1),y_train,x_test.drop('Embarked',axis = 1),ans,str(icount + 2)) ans = RandomForest(x_train.drop('Family_size',axis = 1),y_train,x_test.drop('Family_size',axis = 1),ans,str(icount + 3)) ans = RandomForest(x_train.drop('Fare_group',axis = 1),y_train,x_test.drop('Fare_group',axis = 1),ans,str(icount + 4)) ans = RandomForest(x_train.drop('Age_group',axis = 1),y_train,x_test.drop('Age_group',axis = 1),ans,str(icount + 5)) ans = RandomForest(x_train.drop('Pclass',axis = 1),y_train,x_test.drop('Pclass',axis = 1),ans,str(icount + 6)) ans = RandomForest(x_train.drop('Fare',axis = 1),y_train,x_test.drop('Fare',axis = 1),ans,str(icount + 7)) ans = RandomForest(x_train,y_train,x_test,ans,str(icount + 8)) ans = RandomForest(x_train,y_train,x_test,ans,str(icount + 9)) #ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 13)) #ans = RandomForest(x_train,y_train,x_test,ans,str(len(l) + 14)) ans['Sum'] = ans.apply(lambda x:x.sum(),axis = 1) ans['Sum'] = ans['Sum'] - ans['PassengerId'] ans['Survived'] = ans['Sum'].apply(lambda x: 1 if x &gt;= 41 else 0) #for idx in range(icount + 10): # show_acc(ans,idx) for idx in range(icount + 10): ans.drop(str(idx),inplace = True,axis = 1) ans.drop('Sum',inplace = True,axis = 1) print(ans.head(10)) ans.to_csv('V3.9.csv',index = False) kaggle: 0.80382 此版本基于V2版本的数据，在kaggle上提交大约会在0.794~0.803左右震荡(可能还会再偏高点或偏低点) otherabout IV and WOE: 12345678910111213141516def calcWOE(dataset,col,targe): subdata=df(dataset.groupby(col)[col].count()) suby=df(dataset.groupby(col)[targe].sum()) data=df(pd.merge(subdata,suby,how="left",left_index=True,right_index=True)) b_total=data[targe].sum() total=data[col].sum() g_total=total-b_total data["bad"]=data.apply(lambda x:round(x[targe]/b_total,3),axis=1) data["good"]=data.apply(lambda x:round((x[col]-x[targe])/g_total,3),axis=1) data["WOE"]=data.apply(lambda x:np.log(x.bad/x.good),axis=1) return data.loc[:,["bad","good","WOE"]] def calcIV(dataset): dataset["IV"]=dataset.apply(lambda x:(x.bad-x.good)*x.WOE,axis=1) IV=sum(dataset["IV"]) refer: https://blog.csdn.net/weixin_38940048/article/details/82316900]]></content>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫入门]]></title>
    <url>%2F2019%2F03%2F02%2FPython%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8_1%2F</url>
    <content type="text"><![CDATA[python爬虫入门学了一下爬虫，感觉云玩家和自己上手还是有去别的 真的看看简单，做起来就有点难了 注意爬虫首先我们要分析它的合法性：当然大部分情况下是合法的毕竟搜索引擎就是最大的爬虫啊但是，如果遇到网站上有明确说明的，例如淘宝，那就最好不要去爬了吧，而且淘宝对于爬虫还是有限制的。 1www.taobao.com/robots.txt 在网站后面加 /robots.txt 就可以知道网站会禁止你爬什么内容。上面的网址就说明了淘宝对于各大爬虫的限制 库安装首先爬虫需要正则表达式和网页分析，因此我们需要导入两个库 12import requestsimport re 但是requests需要自己导入。 我是在windows上面操作的，因此这里只讲windows上面的步骤： ​ 1、管理员方式 打开cmd, （ Win+R 输入cmd） ​ 2、输入命令行：python -m pip install –upgrade pip (这个是升级pip，如果你的pip已经是最高级请跳过) ​ 3、输入命令行：pip install requests 然后你就可以看到安装好的界面了 过程分析首先我们要爬取一个网页，那么我们就需要一个网址，因为我比较喜欢看小说，因此就选择了一个小说网站来爬： https://www.biquge5.com/25_25502/ （我选择的是笔趣阁） 我们需要对这个网站进行分析：（鼠标右键 - &gt; 查看网页源代码） 我们可以通过这个源码分析分析出一些东西： 1、找到小说的名称 2、找到小说下面的具体的章节和链接（url）： 3、通过url再进入具体的章节内容，然后再进入网页源代码(第一章为例)： 4、找到其中的正文内容就可以导出来了 当然网页的分析需要正则表达式 正则表达式这里通过 re.findall（） 来实现 12re.findall(p,string)# 以list的形式返回string中所有与p相匹配的字符串 这个需要库re 1import re re.findall ( ) 查找全部r标识代表后面是正则的语句 下面举一个小小的例子： 123import rep = 'hello world'print(re.findall(r'hello',p)) 得到结果： 1['hello'] (.*?)这个可以匹配所有字符(长度不限)： 123import rep = 'hello world'print(re.findall(r'hello(.*?)d',p)) 得到结果： 12[' worl']# 这个得到的是hello和d之间所有的字符 正则end觉得菜鸟教程中的正则表达式写的挺好的，就不写了，偷懒以下。 代码实现稍微正式一点了 请求头爬别人的网站你就是模仿浏览器，模仿浏览器发送http请求，但是你需要一个请求头，表示是模仿请求的，不然可能会被封或者无法访问。（当然，很多网站加不加这个请求头是无所谓的） 我用的是chrome的: 12header = &#123;"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36"&#125;# 不要用 head 定义，否则会报错 requests.get()接下来才是正戏上场 利用requests.get()获得网页上的内容 12345678910111213# 获得一个网址url = 'https://www.biquge5.com/25_25502/'# 请求头，模仿浏览器header = &#123;"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36"&#125;# 发送http请求response = requests.get(url, headers=header)# 编码方式# 关于这个编码，有时候要加上，有时候不要加，自己判断吧# response.encoding = 'utf-8'# 目标网页源码html = response.text# 小说标题 (re.findall()正则表达式是通过数组的方式传的，因此最后要加一个[0])title = re.findall(r'&lt;h1&gt;(.*?)&lt;/h1&gt;',html)[0] 获得章节url和章节名称12chapter_info_list = re.findall(r'(.*?)"&gt;(.*?)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;',html)# 利用正则表达式来获取内容 重复上面的步骤12345678910111213141516171819202122232425262728293031323334353637383940for chapter_info in chapter_info_list: chapter_url,chapter_title = chapter_info # print(chapter_url,chapter_title) chapter_url = "https://www.biquge5.com/25_25502/%s" % chapter_url # 获得章节所在url源码 chapter_response = requests.get(chapter_url,headers=header) # chapter_response.encoding = 'utf-8' chapter_html = chapter_response.text # print(chapter_html) # break # 获得章节内容 chapter_post = re.findall(r'&lt;div id="content"&gt;(.*?)&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;',chapter_html,re.S)[0] # print(chapter_post) # 清洗数据 chapter_post = chapter_post.replace('&lt;br /&gt;','\n') chapter_post = chapter_post.replace('&amp;nbsp;',' ') chapter_post = chapter_post.replace('&lt;script&gt;s1();&lt;/script&gt;','') # 保存数据 fb.write(chapter_title) fb.write('\n') fb.write(chapter_post) if re.findall(r'&lt;font color=red&gt;&lt;b&gt;(.*?)&lt;/b&gt;&lt;/font&gt;',chapter_html): chapter_next = re.findall(r'&lt;font color=red&gt;&lt;b&gt;(.*?)&lt;/b&gt;&lt;/font&gt;',chapter_html)[0]# 为什么用while？因为有时候一个下一页，所以需要while特判是否有下一页 while chapter_next == '下一章': if re.findall(r'&lt;a href="(.*?)"&gt;下一页&lt;/a&gt;',chapter_html): chapter_url = re.findall(r'&lt;a href="(.*?)"&gt;下一页&lt;/a&gt;',chapter_html)[0] chapter_url = "https://www.biquge5.com/25_25502/%s" % chapter_url chapter_response = requests.get(chapter_url,headers=header) chapter_html = chapter_response.text chapter_post = chapter_post = re.findall(r'&lt;div id="content"&gt;(.*?)&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;',chapter_html,re.S)[0] chapter_post = chapter_post.replace('&lt;br /&gt;','\n') chapter_post = chapter_post.replace('&amp;nbsp;',' ') chapter_post = chapter_post.replace('&lt;script&gt;s1();&lt;/script&gt;','') chapter_next = re.findall(r'&lt;font color=red&gt;&lt;b&gt;(.*?)&lt;/b&gt;&lt;/font&gt;',chapter_html)[0] fb.write(chapter_post) fb.write('\n') else: break # print(chapter_post) End第一次爬虫，感觉还是有点有趣，但是不懂得地方还有很多。 本来是打算爬新笔趣阁的，但是新笔趣阁通过requests.get()得到的源码有失真，有一部分正文内容缺失了，估计做了特殊的处理，因此需要后续继续学习，像什么动态网页的爬取之类…………]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸽巢原理]]></title>
    <url>%2F2019%2F02%2F24%2F%E9%B8%BD%E5%B7%A2%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[鸽巢原理如果把 &gt;n 个的元素放入n个抽屉中，那么一定有一个抽屉有大于等于2的元素，故又称 “抽屉原理” poj 2356题意： 找到m个数字使得这m个数字相加为n的倍数 输出m，然后输出这m个数字 题解： 先使用前缀和 sum[] 如果sum[i]%n==0 那么就已经找到了，输出 i 和这 i 个数字就行了 如果sum[i]%n!=0 那么一定存在 sum[j]%n==sum[i]%n , 那么输出i到j之间的数字就行了 注： 我一开始想的时候感觉这个题解有一点点问题，因为这个题解写的是按照连续的数字，也就是求一个区间内的，而且我也没有写0的情况，但是AC了，然后想为什么没有０，为什么数字都在一个区间内？万一不是一个区间的呢？然后就发现其实是我并没有很好的理解鸽巢原理，虽然这个原理看上去很简单，而且看上去和这道题没什么大的联系，但是上面的问题都可以用鸽巢原理来解释。 鸽巢原理对这题的解释： 因为有ｎ个 sum[i] ，所以在 [1,n-1] 的区间内必定最少有一个数字是由多个 sum[i]%n 的情况的。（ 因为sum[i]%n==0 就是一种情况的解，所以区间内不包含0，因此区间是 [1,n-1] ），因此这题必定有解，最少其中一个解必定是连续的。 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;using namespace std;const int N = 1e4 + 5;int a[N],sum[N],mod[N];int main()&#123; int n; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) mod[i]=-1; for(int i=1;i&lt;=n;i++)&#123; cin&gt;&gt;a[i]; sum[i]=sum[i-1]+a[i]; &#125; for(int i=1;i&lt;=n;i++)&#123; if(sum[i]%n==0)&#123; cout&lt;&lt;i&lt;&lt;endl; for(int j=1;j&lt;=i;j++) cout&lt;&lt;a[j]&lt;&lt;endl; return 0; &#125; else&#123; if(mod[sum[i]%n]!=-1)&#123; cout&lt;&lt;i-mod[sum[i]%n]&lt;&lt;endl; for(int j=mod[sum[i]%n]+1;j&lt;=i;j++) cout&lt;&lt;a[j]&lt;&lt;endl; return 0; &#125; &#125; mod[sum[i]%n]=i; &#125; return 0;&#125; poj 3370和上题一样的思路……就不多说了，直接上代码1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;cstdio&gt;using namespace std;const int N = 1e5 + 5;int a[N],sum[N],mod[N];int main()&#123; int n,m; while(~scanf("%d%d",&amp;n,&amp;m)) &#123; if(n==0&amp;&amp;m==0) return 0; for(int i=1;i&lt;=m;i++) mod[i]=-1; for(int i=1;i&lt;=m;i++) sum[i]=0; for(int i=1;i&lt;=m;i++)&#123; scanf("%d",&amp;a[i]); sum[i]=sum[i-1]+a[i]; sum[i]%=n; &#125; for(int i=1;i&lt;=m;i++)&#123; if(sum[i]==0)&#123; for(int j=1;j&lt;=i;j++) printf("%d ",j); break; &#125; else&#123; if(mod[sum[i]]!=-1)&#123; for(int j=mod[sum[i]]+1;j&lt;=i;j++) printf("%d ",j); break; &#125; &#125; mod[sum[i]]=i; &#125; printf("\n"); &#125; return 0;&#125;]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[置换]]></title>
    <url>%2F2019%2F02%2F24%2F%E7%BD%AE%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[置换感觉这类的题目就是找循环节，找到循环节就方便计算了，当然循环节应该是暴力寻找吧………… 置换群 poj 1721题意： 给你一个数组a[]，对数组a进行排序，排序的规则是这样的：第i个位置的数变成a[a[i]]，现在给你一个已经按照这个规则排完s次的数组，问你原数组是什么样的 题解： 这是个置换群，当你进行排序时，你会发现这个是循环的，因此这道题就是找循环节，找到循环节之后就很简单了…… 循环节怎么找，当然是暴力找了，每交换一次比较一次，知道找到循环节 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std;#define ll long longconst int N = 1e3 + 5;int a[N],b[N],c[N];int n;void change()&#123; for(int i=1;i&lt;=n;i++)&#123; b[i]=c[c[i]]; &#125; for(int i=1;i&lt;=n;i++)&#123; c[i]=b[i]; &#125;&#125;int same()&#123; for(int i=1;i&lt;=n;i++)&#123; if(a[i]!=b[i]) return 0; &#125; return 1;&#125;int main()&#123; int s; cin&gt;&gt;n&gt;&gt;s; for(int i=1;i&lt;=n;i++)&#123; cin&gt;&gt;a[i]; c[i]=a[i]; &#125; int sum=0; while(1)&#123; sum++; change(); if(same()) break; &#125; int num=sum-s%sum; while(num--)&#123; change(); &#125; for(int i=1;i&lt;=n;i++) cout&lt;&lt;b[i]&lt;&lt;endl; return 0;&#125; 置换的幂 poj 3128一个置换平方了之后长度为奇数的循环节长度不变，长度为偶数的循环节变成长度相等的两个所以判断平方之后的长度为偶数的循环节个数是否为偶数就行了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;string&gt;#include &lt;queue&gt;using namespace std;#define ll long longconst int N = 30;int a[N];int b[N];int ans[N];int vis[N];void init(int n)&#123; queue&lt;int&gt;q; while(!q.empty()) q.pop(); for(int i=1;i&lt;=n;i++) b[i]=i; for(int i=1;i&lt;=n;i++) ans[i]=0; for(int i=1;i&lt;=n;i++) vis[i]=0; for(int i=1;i&lt;=n;i++)&#123; if(vis[i]) continue; int sum=0; while(1)&#123; b[i]=a[b[i]]; q.push(b[i]); sum++; if(b[i]==i) break; &#125; while(!q.empty())&#123; vis[q.front()]=1; q.pop(); &#125; ans[sum]++; &#125;&#125;int main()&#123; int t; cin&gt;&gt;t; string s; while(t--)&#123; cin&gt;&gt;s; int size=s.size(); for(int i=0;i&lt;size;i++)&#123; a[i+1]=s[i]-'A'+1; &#125; init(s.size()); int flag=1; for(int i=1;i&lt;=size;i++)&#123; if(i%2) continue; if(ans[i]%2)&#123; flag=0; break; &#125; &#125; if(flag) cout&lt;&lt;"Yes"&lt;&lt;endl; else cout&lt;&lt;"No"&lt;&lt;endl; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Euler]]></title>
    <url>%2F2019%2F01%2F30%2FEuler%2F</url>
    <content type="text"><![CDATA[Euler定义欧拉函数是小于x的整数中与x互质的数的个数，一般用φ(x)表示。特殊的，φ(1)=1。 计算φ函数的值： φ(x)=x(1-1/p(1))(1-1/p(2))(1-1/p(3))(1-1/p(4))…..(1-1/p(n)) 其中p(1),p(2)…p(n)为x的所有质因数;x是正整数; φ(1)=1(唯一和1互质的数，且小于等于1)。 注意：每种质因数只有一个。 例如: ​ φ(10)=10×(1-1/2)×(1-1/5)=4; ​ 1 3 7 9 ​ φ(30)=30×(1-1/2)×(1-1/3)×(1-1/5)=8; ​ φ(49)=49×(1-1/7)=42; 性质1.对于质数p，φ(p)=p−1。 2.若p为质数，$n=p^{k}$ ,则 φ(n)=$p^{k}-p^{k-1}$ 3.欧拉函数是积性函数，但不是完全积性函数。若m,n互质，则φ(m∗n)=φ(m)∗φ(n) 。特殊的，当m=2，n为奇数时，φ(2*n)=φ(n)。 4.当n&gt;2时，φ(n)是偶数。 5.小于n的数中，与n互质的数的总和为：φ(n) * n / 2 (n&gt;1)。 6.n=$\sum_{d|n}{φ(d)}$ ，即n的因数（包括1和它自己）的欧拉函数之和等于n 7.欧拉降幂公式：$A^{B} mod C = A^{B mod φ(C) + φ(C) } mod C$ 板子求解欧拉函数值才是关键。 暴力复杂度 O(sqrt(n)) φ(x)=x(1-1/p(1))(1-1/p(2))(1-1/p(3))(1-1/p(4))…..(1-1/p(n)) 1234567891011ll Euler(ll n)&#123; //返回Euler(n) ll res=n,a=n; for(int i=2;i*i&lt;=a;i++)&#123; if(a%i==0)&#123; res=res/i*(i-1);//先进行除法是为了防止中间数据的溢出 while(a%i==0) a/=i; &#125; &#125; if(a&gt;1) res=res/a*(a-1); return res; &#125; 埃拉托斯特尼筛求欧拉函数复杂度 O(n*log(log(n))) 1234567891011121314void euler(int n)&#123; for (int i=1;i&lt;=n;i++) phi[i]=i; for (int i=2;i&lt;=n;i++) &#123; if (phi[i]==i)//这代表i是质数 &#123; for (int j=i;j&lt;=n;j+=i) &#123; phi[j]=phi[j]/i*(i-1);//把i的倍数更新掉 &#125; &#125; &#125;&#125; 欧拉筛求欧拉函数复杂度 O(n) 1234567891011121314151617181920212223int prime[N],phi[N],num;void Euler(int n)&#123; phi[1]=1;//1要特判 for (int i=2;i&lt;=n;i++) &#123; if (flag[i]==0)//这代表i是质数 &#123; prime[++num]=i; phi[i]=i-1; &#125; for (int j=1;j&lt;=num&amp;&amp;prime[j]*i&lt;=n;j++)//经典的欧拉筛写法 &#123; flag[i*prime[j]]=1;//先把这个合数标记掉 if (i%prime[j]==0) &#123; phi[i*prime[j]]=phi[i]*prime[j];//若prime[j]是i的质因子，则根据计算公式，i已经包括i*prime[j]的所有质因子 break;//经典欧拉筛的核心语句，这样能保证每个数只会被自己最小的因子筛掉一次 &#125; else phi[i*prime[j]]=phi[i]*phi[prime[j]];//利用了欧拉函数是个积性函数的性质 &#125; &#125;&#125;]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mobius]]></title>
    <url>%2F2019%2F01%2F30%2FMobius%2F</url>
    <content type="text"><![CDATA[MobiusMobius（莫比乌斯）这东西看得我是真的头痛，网上找了好多博客，只看懂了一点点，完全不会证明，数学太菜了。 推荐一个博客 理论部分莫比乌斯函数莫比乌斯函数( $\mu(d)$ ) 当d == 1时， $\mu(d)=1$ 当d == $p_1$*$p_2$*…*$p_k$ 时, $\mu(d)$ = $(-1)^{k}$ 当d 等于其他情况时，$\mu(d)=0$ $\sum_{d|n}{\mu(d)}=$$\begin{cases}1&amp; (n=1) \\ 0 &amp; (n&gt;1)\end{cases}$ $\sum_{d|n}{\frac{\,u(d)}{d}}=\frac{\phi(n)}{n}$ 莫比乌斯反演关于莫比乌斯反演，其实就是求的关于 F(n) 和 f(n) 之间的关系 由于不会证明，只能提供两个公式了（也许哪天我会证了来补一下，溜…） $F(n)=\sum_{d|n}{f(d)} —&gt; f(n)=\sum_{d|n}{\mu(d)F(\frac{n}{d})}$ $F(n)=\sum_{n|d}{f(d)}—&gt;f(n)=\sum_{n|d}{\mu(\frac{d}{n})F(d)}$ 整除分块整除分块也是一个很重要的东西 引入问题： $\sum_{i=1}^{n}{\frac{n}{i}}$ 对于这个的求解，如果n很大很明显会超时 求解：我们通过打表可以发现其实有很多重复的，那么我们的关键就是计算重复的部分，这样就分成了很多块，我们发现每一块的最后一个数字是 n/(n/i)12345for(int l=1,r;l&lt;=n;l=r+1)&#123; r=n/(n/l); ans+=(r-l+1)*(n/l);&#125; 当然了，在莫比乌斯函数的应用的时候可能也会遇到整除分块的应用。 板子莫比乌斯函数12345678910111213141516171819202122const int N = 1e5 + 5;int mu[N],prime[N],b[N],tot;void getMobius()&#123; mu[1] = 1; //μ函数的1是特殊情况 for(int i = 2; i &lt;= N; ++i)&#123; if(!b[i])&#123; prime[++tot] = i; mu[i] = -1; //质数的μ值一定为-1 &#125; for(int j = 1; j &lt;= tot; ++j)&#123; if(i * prime[j] &gt; N) break; b[i * prime[j]] = 1; if(i % prime[j] == 0)&#123; mu[i * prime[j]] = 0;//i中已经包括了prime[j] break; &#125; else&#123; mu[i * prime[j]] = -mu[i];//不能整除，意味着i中原没有prime[j]这个素因子 &#125; &#125; &#125;&#125; 题目hdu 1695莫比乌斯入门题： 题意要求gcd(x,y)=k的对数，那么我们可以转化一下，变成求gcd(x/k,y/k)==1的对数。 于是就变成了在 [1,b/k] 和 [1,d/k] 之间求 gcd(x,y)==1 的对数（之后的b，d均为除以 k 之后的结果） 再联想到莫比乌斯反演公式的第二条，我们就可以得到两个函数 F(t)=(gcd(x,y)%t==0的对数) f(t)=(gcd(x,y)==t的对数) 接下去我们要求的就是gcd(x,y)==1的对数，那么就是求f（1），这时候就要继续用莫比乌斯反演公式的第二条，刚刚我们用了前半部分，现在我们用后半部分，可以得到f(1)=mu[1]*F(1)+mu[2]*F(2)+…+mu[min(b,d)]*F(min(b,d)) 显然我们可以得到F(t)=(b/t)*(d/t) 但是我们要考虑重复部分，因为在计算的时候是有一部分重复的。 刚才我们得到了f(1)，即区间 [1,b] 和 [1,d] 之间 gcd(x,y)==1 的所有对数，那么重复的部分在哪里呢？当然是区间 [1,min(b,d)] 和 [1,min(b,d)] 之间了，那么答案就出来了，答案为 ans(区间 [1,b] 和 [1,d] ) - ans(区间 [1,min(b,d)] 和 [1,min(b,d)])/2 123456789101112131415161718192021222324// getMobius() 这个直接使用上面的板子getMobius();int t;cin&gt;&gt;t;int a,b,c,d,k;int ca=0;while(t--)&#123; ca++; cin&gt;&gt;a&gt;&gt;b&gt;&gt;c&gt;&gt;d&gt;&gt;k; if(k==0)&#123; //注意0的时候特判一下，不然会re cout&lt;&lt;"Case "&lt;&lt;ca&lt;&lt;": "; cout&lt;&lt;0&lt;&lt;endl; continue; &#125; b/=k; d/=k; ll ans1=0,ans2=0; for(int i=1;i&lt;=min(b,d);i++)&#123; ans1+=(ll)mu[i]*(b/i)*(d/i); //这里注意long long,不然会wa ans2+=(ll)mu[i]*(min(b,d)/i)*(min(b,d)/i); &#125; cout&lt;&lt;"Case "&lt;&lt;ca&lt;&lt;": "; cout&lt;&lt;ans1-ans2/2&lt;&lt;endl;&#125;]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Polya]]></title>
    <url>%2F2019%2F01%2F25%2Fpolya%2F</url>
    <content type="text"><![CDATA[Polya有关于这个polya（波利亚）公式,其实我只能看懂一点，但是解释不了，因此就从网上拉了一篇博客，感觉写的挺好，你们可以借鉴一下 polya 公式方案总数: $\frac{旋转置换+翻转置换}{置换群}$ 旋转置换:方案数: $k^{gcd(n,i)}$ 一个长度为n的环，每 i 个染同一种颜色，可以染多少种颜色。 假设起点在x，则x，x+i，x+2i，……，x+ki，…… 假设在第t次，第一次回到起点，则x=(x+ti)%n =&gt; ti%n=0 =&gt; t=$\frac{LCM(i,n)}{i}$=$\frac{\frac{n*i}{GCD(n,i)}}{i}$=$\frac{n}{GCD(n,i)}$。 那么可以上$\frac{n}{t}$种颜色，即$\frac{n}{\frac{n}{GCD(n,i)}}$种，所以旋转的着色方案有种$k^{gcd(n,i)}$。 翻转置换: 方案数：n$k^{\frac{n+1}{2}}$或者$\frac{n}{2}$$k^{\frac{n}{2}+1}+k^{\frac{n}{2}}$ 当 n 为奇数时，对称轴为 某点-对边中点 ，显然这样置换有 n 种，每个置换有 $\frac{n+1}{2}$ 个循环。因此答案为 n$k^{\frac{n+1}{2}}$；当 n 为偶数时，对称轴为 某点-对点 时，置换有$\frac{n}{2}$ 种，每个置换有$\frac{n}{2}$+1 个循环；对称轴为 某边-对边中点 时，置换有 $\frac{n}{2}$ 种，每种置换有 $\frac{n}{2}$个循环。因此答案为$\frac{n}{2}$$k^{\frac{n}{2}+1}+k^{\frac{n}{2}}$。 题目来几道题目吧 HDU 1817很裸的Polya定理应用，用一下上面的公式就行了 珠子可以染的颜色固定为3种，然后n颗珠子 12345678910111213141516//注意特判0的情况/*quick_power（） 为求幂，可以直接用pow（但是这个要记得类型转换，不推荐使用），也可以用快速幂，本人采用的是快速幂*/if(n==0)&#123; cout&lt;&lt;0&lt;&lt;endl; continue;&#125;m=3;ll sum=0;ll ans=0;for(int i=1;i&lt;=n;i++)&#123; sum=__gcd(n,i); ans+=quick_power(m,sum);&#125;if(n%2) ans=ans+n*quick_power(m,(n+1)/2);else ans=ans+n/2*(quick_power(m,n/2)+quick_power(m,n/2+1));cout&lt;&lt;ans/(n+n)&lt;&lt;endl; POJ 2409很裸的Polya定理应用，用一下上面的公式就行了12345678910/*quick_power（） 为求幂，可以直接用pow（但是这个要记得类型转换，不推荐使用），也可以用快速幂，本人采用的是快速幂*/ll sum=0;ll ans=0;for(int i=1;i&lt;=n;i++)&#123; sum=__gcd(n,i); ans+=quick_power(m,sum);&#125;if(n%2) ans=ans+n*quick_power(m,(n+1)/2);else ans=ans+n/2*(quick_power(m,n/2)+quick_power(m,n/2+1));cout&lt;&lt;ans/(n+n)&lt;&lt;endl; POJ 2154总思路：Polya+欧拉函数 这题要注意忽略的是旋转置换，因此只需要计算旋转置换就行了 12345678910// 这是种超时的写法，因为n太大了cin&gt;&gt;n&gt;&gt;p;m=n;ll sum=0;ll ans=0;for(int i=1;i&lt;=n;i++)&#123; sum=__gcd(n,i); ans+=power_mod(m,sum,p);&#125;cout&lt;&lt;ans/(n)&lt;&lt;endl; 那么我们就需要进行优化了，当然是利用欧拉函数进行优化了 我们计算$n^{gcd(n,i)}$的时候，会发现有很多都是重复计算的，因为gcd(n,i)对于不同的i可能有一样的解，比如gcd(7,2)和gcd(7,3)是一样的，因此我们可以从这上面考虑节省时间。 接下来又是考验数学的时候了： ans = $\frac{1}{n} \sum_{i=1}^{n}{n^{gcd(n,i)}}$ ​ = $\frac{1}{n} \sum_{i|n}{\phi(\frac{n}{i}) n^{i}}$ ​ = $\sum_{i|n}{\phi(\frac{n}{i}) n^{i-1}}$ 然后问题就变成了求 $\phi(\frac{n}{i})$ ，套用一下欧拉函数的板子就行了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748int Euler(int n)&#123; //返回Euler(n) int res=n,a=n; for(int i=2;i*i&lt;=a;i++)&#123; if(a%i==0)&#123; res=res/i*(i-1);//先进行除法是为了防止中间数据的溢出 while(a%i==0) a/=i; &#125; &#125; if(a&gt;1) res=res/a*(a-1); return res; &#125;int power_mod(int a,int b,int c)&#123; int ans=1; a=a%c; while(b!=0) &#123; if(b&amp;1) ans=(ans*a)%c; b&gt;&gt;=1; a=(a*a)%c; &#125; return ans;&#125;int main()&#123; int t; cin&gt;&gt;t; int n,p; int ans=0; while(t--)&#123; cin&gt;&gt;n&gt;&gt;p; ans=0; for(int i=1;i*i&lt;=n;i++)&#123; if(i*i==n)&#123; ans=ans+Euler(n/i)%p*power_mod(n,i-1,p)%p; ans=ans%p; &#125; else if(n%i==0)&#123; ans=ans+Euler(n/i)%p*power_mod(n,i-1,p)%p+Euler(i)%p*power_mod(n,n/i-1,p)%p; ans=ans%p; &#125; &#125; cout&lt;&lt;(ans+p)%p&lt;&lt;endl; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello_myfriends]]></title>
    <url>%2F2019%2F01%2F12%2FHello_myfriends%2F</url>
    <content type="text"><![CDATA[[TOC] hello第一篇也不知道改写什么，花了两天时间搭好了这个博客，看着来吧，先瞎写点什么东西。(看着上面的TOC真难受，也不知道怎么改，想要个目录) 你猜我会不会在这个标题写东西其实我也不知道 听说这是子列表 好吧，上个标题就没打算写东西，测试测试md。 加粗斜线 测试继续接下来测试什么好呢？(列表时记得空一行) A B C 1 2 3 2 3 4 好的，继续这怎么可以没有代码呢12345678#include &lt;bits/stdc++.h&gt;using namespace std;int main()&#123; cout&lt;&lt;"hello world!"&lt;&lt;endl; return 0;&#125; hello everyone 12cout&lt;&lt;&quot;hello world!&quot;&lt;&lt;endl;//也不知道是不是会出现行号 hhh END最后，寒假开始会更新博客，看具体的刷题情况吧（寒假还打算好好的学一下呢）。]]></content>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
